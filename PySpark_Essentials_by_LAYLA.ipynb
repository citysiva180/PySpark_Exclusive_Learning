{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1znNN3wolGhYdmq5mYXbme1_QtRO9VbrV",
      "authorship_tag": "ABX9TyNb0ltKXGrrQrRb1RvyiUZd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/citysiva180/PySpark_Exclusive_Learning/blob/main/PySpark_Essentials_by_LAYLA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORIFqz0yC3Wm",
        "outputId": "5d12d1d5-5c06-4de7-c135-78d193502bc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.9/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317145 sha256=f9cfcc036bb908a329f00b8064ad5c4a7eb498cedfad8d6c9196d00b1890d88b\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/34/a4/159aa12d0a510d5ff7c8f0220abbea42e5d81ecf588c4fd884\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Birds eye view of the course content\n",
        "  - PySpark Dataframes\n",
        "  - Spark MLlib\n",
        "  - Industry Concepts and Best Practices \n",
        "  - All using Real dataseta - no fakes!"
      ],
      "metadata": {
        "id": "VA7zXcXdD6nk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Course Curriculum Overview \n",
        "\n",
        "- Introduction to PySpark\n",
        "- Setup Options and Installation Guides \n",
        "- Transitioning from Python to PySpark\n",
        "\n",
        "## Dataframe Essentials \n",
        "\n",
        "- Read, write and Validata Dataframes \n",
        "- Search/Filter DataFrames \n",
        "- Spark SQL Options\n",
        "- Manipulating Data \n",
        "- Aggregating DataFrames \n",
        "- Joining and Appending DataFrames \n",
        "- Handling Missing Data\n",
        "\n",
        "## MLlib \n",
        "\n",
        "- Foundational Knowledge \n",
        "  - Introduction to Machine Learning \n",
        "  - Introduction to MLlib\n",
        "  - Model Selection, Tuning and Cross Validation\n",
        "- Supervised Learning \n",
        "  - Classification Techniques\n",
        "  - Natural language Processing \n",
        "  - Regression Techniques\n",
        "- Un Supervised Learning \n",
        "  - Clustering and Topic Modelling \n",
        "  - Frequent Pattern Mining\n",
        "\n",
        "## Classification and Algorithm Deep learning \n",
        "\n",
        "- Logistic Regression\n",
        "- Binomial Logistic Regression\n",
        "- Multinomial Logistic Regression\n",
        "- Decision Tree Classifier\n",
        "- Random Forest Classifier \n",
        "- Gradient Boosted Tree Classifier\n",
        "- Multi-Layer perception Classifier\n",
        "- Linear Support Vector Machine \n",
        "- One-vs-Rest Classifer\n",
        "- Naive Bayes\n",
        "\n",
        "## Natural Language Processing \n",
        "- Tokenizing \n",
        "- Cleaning Text\n",
        "- Stopword Removal\n",
        "- Vectorizing Options: \n",
        "  - Count Vectors\n",
        "  - TF-IDF\n",
        "  - Word2Vec\n",
        "\n",
        "## Regression\n",
        "- Linear Regression\n",
        "- Decision Tree Regression\n",
        "- Random Forest Regression\n",
        "- Gradient Boosted Tree Regression\n",
        "\n",
        "## Unsupervised Learning \n",
        "\n",
        "- Clustering\n",
        "  - Latent Drichlet Allocation\n",
        "  - K-Means\n",
        "  - Bisecting K-Means\n",
        "  - Gaussian Mixture Model\n",
        "- Frequent Pattern Mining\n",
        "  - FP-Growth (Frequent Pattern Growth)\n",
        "  - Prefix-Span (Sequential Pattern Mining)\n",
        "\n",
        "## Spark Structured Streaming\n",
        "- Overview of Concepts\n",
        "- Simple exmaple using Sockets \n",
        "- Twitter Project Streaming Live data \n"
      ],
      "metadata": {
        "id": "vt4TCbxfEu5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to PySpark"
      ],
      "metadata": {
        "id": "Va4Lr72aPMM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How is PySpark Different than Python? \n",
        "\n",
        "- Runs on a cluster\n",
        "- Certain Process will look different especially when you get in the Machine learning Libraries \n",
        "- PySpark Does not use Indexing \n",
        "- All Objects in PySpark are Immutable\n",
        "- Error Messags are much less informative \n",
        "- Many of the libraries you are used to using in Python won't function in PySpark"
      ],
      "metadata": {
        "id": "u3vUCIDePlym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### When does data become Big Data? \n",
        "\n",
        "- Datasets which can fit into your disk are not big data \n",
        "- Your data becoms big - if you are unable to perform simple read and aggregations with your un used computable memory \n",
        "- These data would occupy huge memory space which would require higher processing power such as cloud to maintain and tame them \n"
      ],
      "metadata": {
        "id": "xQXr9e0BQMa6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark Benefits\n",
        "\n",
        "- Its Written in Scala...but has plenty of API in Scala, Java, Python and R\n",
        "- Spark Utilizes Hadoop in 2 ways \n",
        "  - Storage\n",
        "  - Process handling \n",
        "- Spark is intended to cover an extensive variety of remaining leads, \n",
        "for example, cluster applications, iterative calculations, intuitive questions and streaming\n",
        "\n",
        "***Lazy Computation***\n",
        "\n",
        "Spark Does not execute the transformation untile some action is encountered or cache is encountered"
      ],
      "metadata": {
        "id": "u77p85E_QyQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spark Core \n",
        "- In charge of Essential IO functionalities \n",
        "- Significant in programming and observing the role of spark cluster \n",
        "- Task Dispatching \n",
        "- Fault Recovery \n",
        "- Over Comes Snag of MapReduce by using In Memory Computation"
      ],
      "metadata": {
        "id": "QygvgqjZQyMj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spark SQL\n",
        "\n",
        "- Distributed framework for structured data processing. \n",
        "- Spark gets more information about the structure of data and the computation. With this information, Spark can perform extra optimization\n",
        "- Great for people who don't know Python, Scala or Java"
      ],
      "metadata": {
        "id": "6aoZhORCQyJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark Streaming\n",
        "\n",
        "- An Add-on to core Spark API \n",
        "  - Scalable \n",
        "  - High Throughput \n",
        "  - fault tolerant stream processing of live data streams \n",
        "- Access data from sources like kafka, Flume, Kinesis or TCP Socket \n",
        "- It can operate using various algorithms uses micro-batching"
      ],
      "metadata": {
        "id": "PUTx7Hn9S4I5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark MLlib\n",
        "\n",
        "- High quality algorithms at high speeds \n",
        "- Make machine learning easy and scalable \n",
        "- Came in Spark Release 2.0 \n",
        "- RDD vs DataFrames"
      ],
      "metadata": {
        "id": "UwrrsZS4S3uk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kSP3ORkcTgC9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}